{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"SZ3TSCtH8k4a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A5LJ6VU3ZiJ1"},"source":["## 1.0 Importando bibliotecas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JOTAi0A_ZlDF"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random\n","import gc\n","\n","# Definindo a semente\n","random.seed(123)"]},{"cell_type":"markdown","metadata":{"id":"-f4j4HyoXhuG"},"source":["## 2.0 Funções auxiliares"]},{"cell_type":"markdown","source":["Nesse trecho, há quatro funções utilitárias para exploração e seleção de variáveis: a generate_metadata(dataframe) cria um DataFrame de metadados (nome da variável, tipo, quantidade e % de nulos, cardinalidade) ordenado por percentual de nulos; remove_highly_correlated_features(df, threshold) calcula a matriz de correlação absoluta, identifica colunas no triângulo superior com correlação acima do threshold e retorna o DataFrame reduzido junto da lista de colunas removidas; amostragem(df, tamanho_amostra) extrai uma amostra reprodutível (random_state=42) do tamanho pedido; e vars_selection(df, percentual_preenchimento, threshold, tamanho_amostragem, chave_principal) combina essas etapas: remove a chave primária temporariamente, cria uma amostra, gera metadados sobre a amostra, filtra variáveis com % de nulos menor ou igual ao percentual_preenchimento, elimina variáveis altamente correlacionadas segundo o threshold, e por fim retorna o df original reduzido apenas às variáveis selecionadas mais a chave_principal. Observação prática: o código chama pod_academy_generate_metadata(amostra) dentro de vars_selection mas a função definida é generate_metadata — ajuste esse nome para evitar erro. Em resumo, essas funções automatizam a análise inicial (missing, cardinalidade), reduzem multicolinearidade e permitem gerar um subconjunto de variáveis pronto para modelagem, controlando preenchimento aceitável, correlação e tamanho da amostra."],"metadata":{"id":"pfUcJyxC8o9g"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZdS3WDDXXJB"},"outputs":[],"source":["# Metadados referente ao conjunto de dados\n","def generate_metadata(dataframe):\n","    \"\"\"\n","    Gera um dataframe contendo metadados das colunas do dataframe fornecido.\n","\n","    :param dataframe: DataFrame para o qual os metadados serão gerados.\n","    :return: DataFrame contendo metadados.\n","    \"\"\"\n","\n","    # Coleta de metadados básicos\n","    metadata = pd.DataFrame({\n","        'nome_variavel': dataframe.columns,\n","        'tipo': dataframe.dtypes,\n","        'qt_nulos': dataframe.isnull().sum(),\n","        'percent_nulos': round((dataframe.isnull().sum() / len(dataframe))* 100,2),\n","        'cardinalidade': dataframe.nunique(),\n","    })\n","    metadata=metadata.sort_values(by='percent_nulos',ascending=False)\n","    metadata = metadata.reset_index(drop=True)\n","\n","    return metadata\n","\n","def remove_highly_correlated_features(df, threshold):\n","  # Calculate the correlation matrix\n","  corr_matrix = df.corr().abs()\n","\n","  # Select the upper triangle of the correlation matrix\n","  upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n","\n","  # Identify columns to drop based on the threshold\n","  to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n","\n","  # Drop the columns\n","  df_reduced = df.drop(columns=to_drop)\n","\n","  return df_reduced, to_drop\n","\n","def amostragem(df,tamanho_amostra):\n","  amostra = df.sample(n=tamanho_amostra,random_state=42)\n","  return amostra\n","\n","def vars_selection(df,percentual_preenchimento,threshold,tamanho_amostragem,chave_principal):\n","  df_aux = df.drop(columns = chave_principal)\n","  amostra = amostragem(df_aux, tamanho_amostragem)\n","  metadata_df = pod_academy_generate_metadata(amostra)\n","\n","  vars = metadata_df[metadata_df.percent_nulos <= percentual_preenchimento]['nome_variavel']\n","  df_reduced, dropped_features = remove_highly_correlated_features(amostra[vars], threshold=threshold)\n","  vars_selected = df_reduced.columns.to_list()\n","  vars_selected.append(chave_principal)\n","  df_selected = df[vars_selected]\n","  return df_selected"]},{"cell_type":"markdown","metadata":{"id":"2-gaONBKX4Y2"},"source":["## 3.0 Caminhos dos dados"]},{"cell_type":"markdown","source":["Nesse trecho, o código realiza a leitura de três bases de dados distintas em formato CSV: df_treino, com os dados originais de aplicação de crédito; df_dados_internos, contendo as informações agregadas do histórico de aplicações anteriores; e df_dados_externos, com variáveis derivadas de fontes externas (como histórico de crédito em outras instituições). Essas bases são carregadas com o pandas.read_csv() para posterior integração e análise, servindo de base para o desenvolvimento e treinamento do modelo de crédito."],"metadata":{"id":"CcTe8x0n9EBg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0WC_EcSX6ve"},"outputs":[],"source":["df_treino = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Projetos/Modelo_Credito_Application /Conjunto_de_Dados/application_train.csv')\n","df_dados_internos = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Projetos/Modelo_Credito_Application /Conjunto_de_Dados_Tratados/previous_application_completo_agg_selected.csv')\n","df_dados_externos = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Projetos/Modelo_Credito_Application /Conjunto_de_Dados_Tratados/bureau_agg_selected.csv')"]},{"cell_type":"markdown","source":["## Join entre as bases"],"metadata":{"id":"Tsbe1_ovhEiK"}},{"cell_type":"markdown","source":["O código une os diferentes conjuntos de dados em um único DataFrame consolidado. Primeiro, o df_treino é combinado com o df_dados_internos por meio de um left join na chave SK_ID_CURR, gerando df1_aux. Em seguida, esse resultado é unido a df_dados_externos, também pela mesma chave, formando o DataFrame final df_treino_completo, que reúne todas as informações internas e externas do cliente. Por fim, os DataFrames intermediários são removidos da memória (del) e o coletor de lixo (gc.collect()) é chamado para liberar espaço e otimizar o uso de memória durante o processamento."],"metadata":{"id":"p9v7e1Am9TIv"}},{"cell_type":"code","source":["df1_aux = pd.merge(df_treino, df_dados_internos, on='SK_ID_CURR', how='left')\n","df_treino_completo = pd.merge(df1_aux, df_dados_externos, on='SK_ID_CURR', how='left')\n","\n","del df_treino, df_dados_internos, df_dados_externos, df1_aux\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r6PWcoaIhGW6","outputId":"4a7d18be-f2c7-4e2a-e7d9-1d157e23b52b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"RGqKGyLGXke6"},"source":["## 4.0 Seleção de variáveis"]},{"cell_type":"markdown","source":["Nesse trecho, o código define a chave principal SK_ID_CURR e aplica a função vars_selection ao DataFrame consolidado df_treino_completo para realizar a seleção automática de variáveis. Essa função filtra as colunas com pelo menos 80% de preenchimento, remove variáveis altamente correlacionadas (acima de 0,5) e utiliza uma amostra de 85.000 registros para acelerar o processo. O resultado (df_selected) é salvo em formato CSV como treino_full.csv, contendo apenas as variáveis mais relevantes para modelagem. Em seguida, a lista de colunas selecionadas é armazenada em variaveis_selecionadas, e os DataFrames temporários são excluídos da memória, com gc.collect() sendo chamado para liberar espaço."],"metadata":{"id":"IK6KGIgj9hjP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yw9lhhzqbx3K","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e4ecf34b-9532-4156-983f-0ea8d94443c9"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-ed59c3b07e44>:25: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n","  corr_matrix = df.corr().abs()\n"]},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":5}],"source":["chave_principal = 'SK_ID_CURR'\n","df_selected = vars_selection(df_treino_completo,percentual_preenchimento=80,threshold=0.5,tamanho_amostragem=85000,chave_principal=chave_principal)\n","df_selected.to_csv(f'/content/drive/MyDrive/Colab Notebooks/Projetos/Modelo_Credito_Application /Conjunto_de_Dados_Tratados/variaveis_selecionadas/bases_finais_treino_teste/treino_full.csv', index=False)\n","\n","variaveis_selecionadas = (df_selected.columns).to_list()\n","\n","del df_selected, df_treino_completo\n","gc.collect()"]},{"cell_type":"markdown","source":["Por fim, é feito omesmo processo de integração feito para a base de treino, mas agora aplicado ao conjunto de teste. Primeiro, ele lê os dados originais de teste e os dados internos e externos já tratados, unindo tudo pela chave SK_ID_CURR com joins à esquerda, formando o DataFrame consolidado df_teste_completo. Em seguida, remove a variável TARGET, que só existe na base de treino, e seleciona apenas as colunas presentes em variaveis_selecionadas, garantindo que treino e teste possuam o mesmo conjunto de variáveis. Por fim, o resultado é salvo em teste_full.csv, enquanto os DataFrames intermediários são deletados e o coletor de lixo é acionado para liberar memória."],"metadata":{"id":"8B7tWG7U9qI0"}},{"cell_type":"code","source":["df_teste = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Projetos/Modelo_Credito_Application /Conjunto_de_Dados/application_test.csv')\n","df_dados_internos = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Projetos/Modelo_Credito_Application /Conjunto_de_Dados_Tratados/variaveis_selecionadas/previous_application_completo_agg_selected.csv')\n","df_dados_externos = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Projetos/Modelo_Credito_Application /Conjunto_de_Dados_Tratados/variaveis_selecionadas/bureau_agg_selected.csv')\n","\n","df2_aux = pd.merge(df_teste, df_dados_internos, on='SK_ID_CURR', how='left')\n","\n","del df_teste, df_dados_internos\n","gc.collect()\n","\n","df_teste_completo = pd.merge(df2_aux, df_dados_externos, on='SK_ID_CURR', how='left')\n","\n","del df_dados_externos, df2_aux\n","gc.collect()\n","\n","variaveis_selecionadas.remove(\"TARGET\")\n","df_selected_teste = df_teste_completo[variaveis_selecionadas]\n","df_selected_teste.to_csv(f'/content/drive/MyDrive/Colab Notebooks/Projetos/Modelo_Credito_Application /Conjunto_de_Dados_Tratados/teste_full.csv', index=False)"],"metadata":{"id":"RdvniNsl43-t"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
